早些时候，TensorFlow主要用来构建相对简单的神经网络模型，对应的图结构也相对简单，较少用到流程控制语句。例如构建`LeNet`、`AlexNet`、`ResNet`等模型时，几乎用不到流程控制功能。表现在图结构上就是有向无环图，我们也称之为静态图。随着深度学习快速发展，在递归神经网络、目标检测神经网络等模型中越来越多的用到了流程控制功能，也就是构建动态图的需求越来越大。因而到目前为止流程控制已经成为现代机器学习算法库的必备功能。

TensorFlow的流程控制实现在不同模式下有不同的方法，大致上来看可以分图模式下的流程控制、命令模式下的流程控制以及`AutoGraph`模式下的流程控制。第一种流程控制方法一般只在图模式（符号式编程模式）下使用，用法较为复杂，不适合在需要使用较多的流程控制模型中使用；后两种流程控制方法的用法简单，并且与Python代码兼容，是面向未来的流程控制方法。本节主要介绍第一种方法的使用方法，另外两种主要在介绍`Eager`模式以及`AutoGraph`模式时详细说明。

TensorFlow流程控制相关的API主要有：

* `tf.identity`
* `tf.tuple`
* `tf.group`
* `tf.no_op`
* `tf.count_up_to`
* `tf.cond`
* `tf.case`
* `tf.while_loop`

## 1. 比较运算Op

与流程控制经常一起出现的是比较运算，比较运算常常出现在流程控制的条件中。比较运算也是一个节点。TensorFlow中实现比较运算的API包括：

- `tf.equal`：逐元素判等，相等返回`True`，否则返回`False`，返回的张量与输入的`shape`相同，类型为`DT_BOOL`。**不支持`==`运算符重载**。
- `tf.not_equal`：逐元素判等，相等返回`False`，否则返回`True`，返回的张量与输入的`shape`相同，类型为`DT_BOOL`。**不支持`!=`运算符重载**。
- `tf.less`：逐元素判断是否`x<y`，小于则返回`True`，否则返回`False`，返回的张量与输入的`shape`相同，类型为`DT_BOOL`。支持`<`运算符重载。
- `tf.less_equal`：逐元素判断是否`x<=y`，小于等于则返回`True`，否则返回`False`，返回的张量与输入的`shape`相同，类型为`DT_BOOL`。支持`<=`运算符重载。
- `tf.greater`：逐元素判断是否`x>y`，大于则返回`True`，否则返回`False`，返回的张量与输入的`shape`相同，类型为`DT_BOOL`。支持`>`运算符重载。
- `tf.greater_equal`：逐元素判断是否`x>=y`，大于则返回`True`，否则返回`False`，返回的张量与输入的`shape`相同，类型为`DT_BOOL`。支持`>=`运算符重载。

`tf.where`：是一种特殊的比价运算符，他根据判别条件，分别从两个张量中抽取子元素组成新的张量，例如：

~~~python
with tf.Session() as sess:
    res = tf.where(tf.constant([True, False]), 
         tf.constant([1, 1]), 
         tf.constant([2, 2]))
    print(sess.run(res))  # >>> [1, 2]
~~~

**注意**：`tf.where`不传入第二、三个参数时，返回判断条件为真的每个元素的索引。

**注意**：比较运算中，参与比较的元素必须具有完全相同的`dtype`，否则无法进比较。

***

**小练习**：

`smooth_l1`是`Fast RCNN`模型中提出的一种多变量回归的代价函数，也是目前常用的代价函数之一，其形式为：
$$
\mathrm{smooth}_{L_1}=
\begin{equation}

\left\{
\begin{aligned}
0.5x^2 && \mathrm{if} \left| x \right| < 1 \\
\left| x \right| - 0.5 &&  \mathrm{otherwise,}
\end{aligned}
\right.

\end{equation}
$$
请利用比较运算操作，实现一函数能对任意张量计算`smooth_l1`的值。

## 2. 张量拷贝

`tf.identity`是用于创建张量副本的虚节点，即生成一个与输入张量相同`shape`与内容（元素）的新张量，其用法如下：

~~~python
tf.identity(input, name=None)
~~~

当`tf.identity`输入与输出的张量在同一个设备上时（例如都在一个CPU上），实际仅仅占用了一个张量内容的内存，这也是称其为虚节点的原因。但如果`tf.identity`输入与输出的张量并不在同一个设备上时，就会拷贝一份内容到新设备中。

**注意**：`tf.Variable`创建变量时，变量需存储在一个设备中，就是通过`tf.identity`实现的，并且默认的创建的变量只存在与一个设备中，这样可以避免浪费存储空间。如果有需要，也可以使用`tf.identity`将变量拷贝在别的设备中。

***

**小练习**：

思考：如下代码是否能够输出期望的`1、2、3、4、5`五个结果？为什么？

~~~python
x = tf.Variable(0.)
x_plus_1 = tf.assign_add(x, 1)

with tf.control_dependencies([x_plus_1]):
    y = x

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for i in range(5):
        print(sess.run(y))
~~~

之前，我们在讲控制依赖时曾提到，只有定义到控制依赖作用域下的代码才会有控制依赖的关系。上述代码中`y=x`语句的含义是将Python变量`y`指向`x`所代表的变量，所以在控制依赖作用域中并没有添加节点，所有执行`y`并不会有对应的依赖节点的执行。

解决上述问题的方法很简单，只需要使用`y=tf.identity(x)`替代`y=x`即可，因为`y=tf.identity(x)`是在控制依赖的作用域中定义的节点。

## 3. 张量结组

`tf.group`可以用于将一系列`op`、`Tensor`组合成为一个`op`，方便统一操作，`tf.group`没有返回值。例如我们需要在执行一个加法操作之前对所有的变量进行一个赋值：

~~~python
a = tf.Variable(0.)
b = tf.Variable(0.)
c = tf.Variable(0.)

assign_a = a.assign_add(1.)
assign_b = b.assign_sub(1.)
assign_c = c.assign_add(2.)

assign_op = tf.group([assign_a, assign_b, assign_c])
d = a + b + c

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
	sess.run(assign_op)
    sess.run(d)  # >>> -7
~~~

##4. 双分支选择结构 

`tf.cond`用来构建（双分支）选择结构，类似于Python中的`if...else...`语句。由于`tf.cond`本身只是一个函数，这回使得构建的选择结构看起来很不直观。`tf.cond`介绍如下：

~~~python
# 返回某一个分支的执行结果，两个分支返回的Tensor数量必须一样多
# 但形状、类型等可以不一致
tf.cond(
    pred,  # `shape=[]`的DT_BOOL类型的Tenor判断条件
    true_fn=None,  # `pred`为True时执行的函数
    false_fn=None,  # `pred`为False时执行的函数
    strict=False,  
    name=None)
~~~

`tf.cond`的判断条件可以是张量，也可以是变量。当执行图时，每一次的执行，选择的分支均与判断条件中此时的值有关系。例如，根据条件变量值来确定执行那个分支：

~~~python
cond = tf.Variable(True)
def true_fn():
    return tf.constant(1)

def false_fn():
    return tf.constant(0)

result = tf.cond(cond, true_fn, false_fn)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    print(sess.run(result))  # >>> 1
    sess.run(cond.assign(False))
    print(sess.run(result))  # >>> 0
~~~

上述代码中的`true_fn`与`false_fn`均可以使用`lambda`表达式替代，例如`lambda: tf.constant(1)`。

***

**小练习**：

复现上述选择结构中的实例代码。

## 5. 多路分支选择结构

使用`tf.cond`无法直接实现多路分支选择结构，然而每个多分支结构都可以写成多个二分支结构的等价形式，所以在多路分支的情况下，使用`tf.cond`也能实现。

例如，实现一种阶跃函数：
$$
y=

\begin{equation}

\left\{
\begin{aligned}
1 && x > 0 \\
0 && x = 0 \\
-1 && x < 0,

\end{aligned}
\right.

\end{equation}
$$

~~~python
def step(x):
    x = tf.convert_to_tensor(x)
    
    def greater_zero_fn():
        return tf.constant(1)
    def equal_zero_fn():
        return tf.constant(0)
    def less_zero_fn():
        return tf.constant(-1)
    
    less_equal = tf.cond(x < 0, less_zero_fn, equal_zero_fn)
    result = tf.cond(x > 0, greater_zero_fn, lambda: less_equal)
    
    return result
~~~

上面的利用2个2路分支结构的写法较为繁琐，并不直观。

一般的多路分支选择结构使用`tf.case`来实现，类似于Python中的`if...elif...else...`。上述阶梯函数使用`tf.case`的实现如下：

~~~python
def step_case(x):
    x = tf.convert_to_tensor(x)
    
    case_greater = (x > 0, lambda: tf.constant(1))
    case_equal = (tf.equal(x, 0), lambda: tf.constant(0))
    case_less = (x < 0, lambda: tf.constant(-1))
    
    return tf.case([case_greater, case_equal, case_less])
~~~

`tf.case`的一个参数，接收一个由`case`项组成的`list`，每个`case`项都是由条件与执行函数组成的元组。

`tf.case`的详细用法如下：

~~~python
tf.case(
    pred_fn_pairs,  # case项组成的list
    default=None,  # 默认值，当case项都不匹配时，执行此出传入的函数
    exclusive=False,  # 为False则执行到第一个匹配条件并执行后结束；为True会判断所有case条件，并且不允许出现多个匹配条件，即要求条件互斥。
    strict=False, 
    name='case')
~~~

## 6. 循环结构

TensorFlow中的循环结构使用`tf.while_loop`构建，类似于Python中的`while`、`for`语句。`tf.while_loop`的基本用法如下：

~~~python
# 当`cond`为`True`时执行`body`
tf.while_loop(
    cond,  # 循环条件（一个函数或lambda表达式），为`True`则继续执行循环
    body,  # 循环执行的内容
    loop_vars,  # `cond`、`body`函数的参数列表
    shape_invariants=None,  # shape不变性
    parallel_iterations=10,  # 允许并行迭代的次数
    back_prop=True,  # 是否为此循环开启反向传播
    swap_memory=False,  # 是否允许`Tensor`在不同设备之间交换，允许的话，可以使得内存不足的设备将张量放在别的设备中
    name=None, 
    maximum_iterations=None, 
    return_same_structure=False)
~~~

`tf.while_loop`实现实现循环结构的方式比较抽象，为了说明其功能，首先我们使用Python完成一个简单循环结构：如下：

~~~python
i = 0
n = 10
while(i < n):
    i = i + 1
~~~

上述结构使用`tf.while_loop`实现的方法如下：

~~~python
def cond(i, n):
    return i < n

def body(i, n):
    i = i + 1
    return i, n

i = tf.constant(0)
n = tf.constant(10)

result_i, result_n = tf.while_loop(cond, body, [i, n])
~~~

代码中定义了两个函数`cond`、`body`，分别用来判断条件与执行循环体，同时可以看到两个函数拥有相同的输入，但`cond`的输出为`DT_BOOL`类型，`body`输出与输入结构相同。循环的执行逻辑是首先把参数`[i, n]`两个张量传入`coord`判断条件是否成立，若不成立则结束循环，否则将`[i, n]`传入`body`中，循环体执行完毕之后，需要返回操作之后的所有张量，这样在第二轮的循环中就会使用第一轮`body`的返回值输入继续判断条件，执行循环体。

`tf.while_loop`的使用有很多需要注意的地方，例如我们需要使用一个变量记录循环的次数，试想使用如下代码执行是否可行：

~~~python
# i用于记录训练次数
i = tf.constant(0)
n = tf.constant(10)
# 定义一个变量也用来记录循环次数，每次body执行，就加1
run_times = tf.Variable(0)  

def cond(i, n):
    return i < n

def body(i, n):
    i = i + 1
    # 给var增量赋值
    global run_times
    run_times.assign_add(1)
    return i, n

result_i, result_n = tf.while_loop(cond, body, [i, n])
~~~

上述代码如果在循环执行结束后，打印变量`var`的值会发现仍然是`0`，这是因为操作`var.assign_add(1)`与`body`中的其它操作没有依赖关系，所以可以加入控制依赖解决问题，如下：

~~~python
i = tf.constant(0)
n = tf.constant(10)
# 定义一个变量用来记录循环次数
run_times = tf.Variable(0)  

def cond(i, n):
    return i < n

def body(i, n):
    # 给var增量赋值
    global run_times
    assign_op = run_times.assign_add(1)
    # 添加控制依赖
    with tf.control_dependencies([assign_op]):
        i = i + 1
    return i, n

result_i, result_n = tf.while_loop(cond, body, [i, n])
~~~

除此以外，还需要注意的是，`tf.while_loop`的第三个参数即`cond、body`函数的输入参数，可以输入常量、变量，但如果输入变量了，那么这意味着使用了变量的初始值作为了输入张量，也就是说变量是没有真正输入进去的。

***

**小练习**：

尝试将上述代码中的常量`i`换做变量，使用`i`记录变量的值，以验证`tf.while_loop`的第三个参数即`cond、body`函数的输入参数接受变量输入是无法改变变量的值的。

## 7. Python、TensorFlow中的流程控制对比

思考：为什么我们不使用Python便利的流程控制，而使用TensorFlow内置的蹩足的流程控制呢？

首先，流程控制是需要用到很多比较运算的，由于`Tensor`比较的结果仍然是`Tensor`，导致Python中的流程控制无法正常工作，例如：

~~~python
a = tf.constant(False)
assert a != False
~~~

当然，如果开启了会话，可以得到`a`的结果再进行比较，但这是混乱的、低效。试想一下一旦需要用到比较运算（事实上不止比较运算会用到）就需要开启会话，那么代码结构杂乱无章，且频繁交互带来了性能问题。

其次，我们需要知道在Python环境中编写的图事实上都是符号，图的逻辑都是在内核中执行完成的。在Python中定义的符号会被送入内核执行环境，而诸如Python中的`if`语句并不是TensorFlow认识的符号，所以并不会送入其内核执行环境中。

例如，我们使用Python中的`if`参与图的定义一个选择结构：

~~~python
a = ...

if a is False:
    const = tf.constant(0.)
else
	const = tf.constant(1.)
   
result = tf.multiply(5, const)
~~~

上述代码如果在构建阶段`a`的值为`False`，则在会话的一次执行中最终`else`分支的子图永远不会参与`multiply`的运算。这就无法达成我们希望根据`a`动态改变分支的目的。

最后，在使用TensorFlow的PythonAPI构建流程控制本质上是构建了一系列图节点，即流程控制是图的一部分，而Python中的流程控制与图没有直接关系。从这个层面看，TensorFlow更像是一个提线木偶，而线是Python，执行者是TensorFlow，线无法代替木偶做任何操作。

## 其它

* `tf.tuple`：可以将输入的多个张量组成一个元组。
* `tf.no_op`：不做任何操作的`op`。
* `tf.count_up_to`：控制一个`DT_INT32`或`DT_INT64`类型变量随着会话执行的次数而逐次增加，直到到达设置的上限为止。

## 作业

1. 设计一个函数，要求输入两个`shape、dtype`一样的张量，输出一个同样`shape、dtype`的张量，并且输出的张量中的元素的每一个值都是输入的两个张量中对应元素最大的。即模拟`tf.maximum`的功能，但不能直接使用此函数。
2. 了解`tf.minimum`、`tf.maximum`的用法。
3. 了解`TensorArray`的基本用法，并尝试配合`tf.while_loop`配合使用。

